## 分析数据

> 文档 ===> 分析 ===> 倒排索引
>
> ​					|
>
> ​					|
>
> ​					|
>
> ​				字符过滤 ===> 文本切分为分词 ===> 分词过滤 ===> 分词索引

### 分析器

- 字符过滤（character filters）
  - 字符转化（如 & ==> and）
  - 停用词（stopword）：如 and/the/to
- 切分为分词（tokenziers）
  - 任意数量

- 分词过滤器（token filters）
  - 一个或多个
  - 如：小写过滤器

- 分词索引
  - 分词 ===> Lucene 索引（倒排索引）
- 设置分析器
  - 创建索引时设置（常用）
  - 在 ES 配置文件中设置全局分析器

### 内置的分析器、分词器、分词过滤器

- 内置的分析器：
  - standard、simple、whitespace、stopword、keyword、pattern、snowball....
- 内置分词器：
  - standard、keyword、letter、lowercase、whitespace、pattern、UAX URL email....
- 分词过滤器：
  - synonym（同义词）

### IK 分词器

- ik_smart：
- ik_max_word：
- 添加字典：config 目录下添加 xxx.dic，修改配置文件；重启 ES

### N 元语法

- N 元语法：如 3 元语法
  - spaghetti ===> spa、pag、agh、ghe、het、ett、tti
  - 设置 min_gram / max_gram ====> 最小/最大 N 元语法
  - 可支持模糊查询 ===> 指定单词的编辑距离来匹配 ===> 确保测试过查询的相关性
- 侧边 N 元语法过滤器
  - 只从单词的头部或结尾开始。
  - 如 min_gram = 2, max_gram = 6
- 滑动窗口分词过滤器
  - 分词级别的 N 元语法
  - 设置 min_single_size(最小为 2) / max_single_size

### 提取词干

- 将单词缩减到基本或词根的形式。
- 使用算法
  - snowball、porter stem、kstem、stemmer
- 使用字典
  - hunspell 分析器 + 字典
- 重写分词过滤器
  - keyword marker / stemmer override 分词过滤器 ===> 放在其他词干过滤器之前



## 相关性搜索

### 打分机制

#### TF-IDF

- TF-IDF ===> TF：词频	IDF：逆文档频率
  - TF：一个词条在文档中出现的次数
  - IDF：如果一个分词在索引的不同文档中出现次数越多，则越不重要（只检查是否出现在某文档中，不检查在该文档出现的次数）
- Lucene 评分公式
  - 调和因子
  - 查询标准化

#### BM25

- k1：词频的重要性（默认为 1.2）
- b：文档篇幅的影响程度（默认为 0.75）
- discount_overlaps：在某个字段中，多个分词出现在同一个位置，是否应该影响长度的标准化。（默认为 true）

#### boosting

- 用来修改文档相关性的程序。
- 两种方式提升文档的得分：
  - 索引文件：需要重新索引文档，不推荐
  - 查询文件（推荐）：可跨越多个字段的查询
- 不是精确的乘数，而是被标准化的。

#### “解释”评分

- 只有在调试查询是使用
- 解释不匹配的原因（已弃用）

#### 再打分

- match ===> rescore
  - 先用更为经济的 match 匹配查询进行搜索，再只对检索到的 hits 执行打分操作。
- 减小评分操作的性能影响

#### function_score（定制得分）

- 使用方式：functions[] ===> 可指定任意数量的任意函数，让它们作用于匹配了初始查询的文档，修改其得分。
- weight 函数：将得分乘以一个常数
- 合并得分：
  - score_mode：处理不同函数得分的合并。
  - boost_mode：原始查询得分 + 函数得分 的合并（默认相乘）
- 定制方式：
  - field_value_factor 函数：根据字段取值修改得分 ===> 加载到内存中，很快
  - script_score：Groovy 写的脚本，用户可完全控制如何修改得分 ===> 每篇查询文档动态执行，较慢
  - random_score：随用不同的随机种子，给予用户为文档指定随机分数的能力。
  - 衰减函数：
    - linear：线性
    - gauss：高斯
    - exp：指数
    - 配置选项：
      - origin：曲线的中心点
      - offset：分数开始衰减的位置，和原点之间的距离。
      - scale、decay：字段值到达指定的 scale 值时，得分减少到指定的 decay。

#### 字段数据

- 字段数据缓存（内存型）：通过预热器预先加载数据
  - 使用场景：
    - 比较和分析操作 ===> 大量数据	===> 访问内存
    - 排序和聚集（常用）
- 管理字段数据
  1. 限制字段数据使用的内存量（es.yml 中设置，重启 ES）
     - **限制字段数据缓存大小（优先）**
     - 设置字段数据失效的过期时间
  2. 使用字段数据的断路器
     - 内存达到上限时启动（数据加载之前预估） ===> 抛出异常
     - 节点运行时，断路器的限制可以动态调整
  3. 使用文档值来避免内存的使用
     - 可以磁盘读取
     - 可以混合使用文档值的字段和内存字段数据缓存的字段
     - 使用有利有弊。



## 聚集

具体结构：

- 需要给每个聚集起一个名字，指定它的类型以及和该类型相关的选项。
- 它们运行在查询的结果之上。
- 可以进一步过滤查询的结果，而不影响聚集。

### 度量聚集

- 从不同文档（其他聚集的文档桶）的分组中提取统计数据
- 不同方式：
  - 可使用脚本 ===> 性能慢
  - stats：基础统计，如 avg、sum...
  - extended_stats：高级统计，如 方差、平方...
  - 近似统计：
    - 百分位（percentiles）：
      1. 指定百分比出现的数值
      2. 某项数值以下出现的百分比，可指定范围
    - 基数（cordinality）：集合中非重复元素的个数。
      - HyperLogLog 算法：将希望分析的字段值进行散列，使用散列值来近似基数。
      - 可设置内存使用量阈值，阈值越高，结果越精准，但消耗的内存越多。



### 多桶型聚集

- 词条聚集（terms）：让用户得知文档中每个词条的频率。
  - 字符串
  - 非分析型
  - 显著词条：significant_terms（数组类型）
    - 哪些词条比通常情况有更高的词频。
- 范围聚集（range）：根据文档落入哪些数值、日期、IP 地址的范围来创建不同的桶。
  - 数值型字段
  - 不必连续，可以是分离的，也可以是重叠的。
  - date_range
- 直方图聚集（histogram）：和范围聚集类似。
  - 固定间距 
  - 日期：date_histogram
- 嵌套聚集（nested）、反嵌套聚集
- 地理距离聚集（geo distance）

### 单桶聚集

- global 聚集：全局聚集
- filter 和 filters
  - 限制聚集所统计的文档，而不会影响查询结果。
- missing 聚集
  - 缺乏某个特定字段 ===> 未设置值的字段。



## 文档间的关系

